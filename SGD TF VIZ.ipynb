{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Intro\n",
    "In current notebook I show implementation of Gradient Descent (GD) with tensorflow for Simple Linear Regression and visualize the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(intercept=1, slope=0.8, n=50, normalize=False):\n",
    "    np.random.seed(11)\n",
    "    x = np.random.uniform(low=-1, high=1, size=n)\n",
    "    noise_scale = (x.max()-x.min()) / 10\n",
    "    noise = np.random.normal(loc=0, scale=noise_scale, size=n)\n",
    "    y = intercept + slope*x + noise\n",
    "    # normalization #\n",
    "    # Using non normalized data cause to bad SGD convergence. \n",
    "    if normalize:\n",
    "        norm = lambda vec: (vec - vec.mean()) / vec.std()\n",
    "        x, y = norm(x), norm(y)\n",
    "    return x, y\n",
    "x, y = generate_data(normalize=True) \n",
    "data_df = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "# data visualisation\n",
    "sns.set(style=\"ticks\")\n",
    "g = sns.JointGrid(x='x', y='y', data=data_df, size=5, ratio=4)\n",
    "_ = g.plot_joint(sns.regplot, marker='P', ci=None, line_kws={'ls':'--', 'lw':1})\n",
    "_ = g.plot_marginals(sns.distplot, bins=10)\n",
    "\n",
    "# simple linear regression solution\n",
    "reg_sol = stats.linregress(x, y)\n",
    "SSE = lambda y, y_pred: np.sum((y-y_pred)**2)\n",
    "# y_pred = b_0 + b_1*x\n",
    "y_pred = reg_sol.intercept + reg_sol.slope * x\n",
    "print('Liner Regression Solution:\\n b_0=%.3f b_1=%.3f\\n SSE=%.2f' %(reg_sol.intercept, reg_sol.slope, SSE(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_solution (x_data, y_data, n_iter, learning_rate, print_st=1):\n",
    "    # slope and intercept initial value\n",
    "    init_val = -2.0\n",
    "    b_0 = tf.Variable(init_val, tf.float32)\n",
    "    b_1 = tf.Variable(init_val, tf.float32)\n",
    "    \n",
    "    # Linear Model definition\n",
    "    x = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    lm = b_0 + b_1*x\n",
    "    \n",
    "    # SSE loss  - the same metric as Linear Regression use \n",
    "    sse_loss = tf.reduce_sum(tf.square(lm-y))\n",
    "    # optimizer\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # opt = tf.train.AdagradOptimizer(learning_rate)   # possible to use different GD variations\n",
    "    # opt = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = opt.minimize(sse_loss)\n",
    "    \n",
    "    # training #\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    ans = []  # intermidiate values of estimated b0, b1 and SSE value\n",
    "    ans.append(sess.run([b_0, b_1, sse_loss], {x:x_data, y:y_data})) # initial values\n",
    "    print('initial state')\n",
    "    print('b0=%.3f  b1=%.3f  sse=%.3f\\n' %(ans[0][0], ans[0][1], ans[0][2]))\n",
    "    # perform SGD iterations \n",
    "    for i in range(n_iter):\n",
    "        sess.run(train, {x:x_data, y:x_data})\n",
    "        if i%print_st==0:\n",
    "            ans_i = sess.run([b_0, b_1, sse_loss], {x:x_data, y:y_data})\n",
    "            print('iter %d' %(i+1))\n",
    "            print('b0=%.3f  b1=%.3f  sse=%.3f' %(ans_i[0], ans_i[1], ans_i[2]))\n",
    "            ans.append(ans_i)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_ans = sgd_solution(x, y, n_iter=10, learning_rate=0.005, print_st=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "          \n",
    "def plot_sse(x, y, sgd_ans):\n",
    "    # Prepare data for ploting #\n",
    "    # grid of b0, b1 values\n",
    "    b0 = np.arange(-2, 3, .01)\n",
    "    b1 = np.arange(-2, 3, .01)\n",
    "    b0_, b1_ = np.meshgrid(b0, b1)\n",
    "    \n",
    "    # calculate SSE values for each values b0 and b1\n",
    "    sse_vals = np.zeros((len(b0), len(b0)))\n",
    "    for i in range(len(b0)):\n",
    "        for j in range(len(b0)):\n",
    "            y_predict = b0_[i][j] + b1_[i][j]*x\n",
    "            sse_vals[i][j] = SSE(y, y_predict)\n",
    "    \n",
    "    # Plot #\n",
    "    fig = plt.figure(figsize=(12, 8)) # with/ high\n",
    "    ax = fig.gca(projection='3d')\n",
    "    # SSE surface\n",
    "    surf = ax.plot_surface(b1_, b0_, sse_vals,\n",
    "                    alpha=0.35, cmap=cm.coolwarm, linewidth=0, antialiased=False,\n",
    "                    rstride=20, cstride=20)\n",
    "    # SSE contours\n",
    "    cset = ax.contour(b1_, b0_, sse_vals, zdir='z',\n",
    "                      levels=[15, 50, 125, 200, 300],\n",
    "                      offset=-200, cmap=cm.coolwarm)\n",
    "    # plot labels\n",
    "    ax.set_xlabel('b1', size='large', labelpad=10)\n",
    "    ax.set_xlim(-2, 3)\n",
    "    ax.set_ylabel('b0', size='large', labelpad=10)\n",
    "    ax.set_ylim(-2, 3)\n",
    "    ax.set_zlabel('SSE', size='large', labelpad=10)\n",
    "    ax.set_zlim(-200, 1000)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=30, pad=0.01)\n",
    "    \n",
    "    # Gradient iterations - black triangles #\n",
    "    sgd_ans = np.array(sgd_ans)\n",
    "    sgd_b0, sgd_b1, sgd_sse = sgd_ans[:,[0]], sgd_ans[:,[1]], sgd_ans[:,[2]]\n",
    "    ax.plot(sgd_b1, sgd_b0, [-200]*len(sgd_b0), '>k', label='GD iterations')\n",
    "    \n",
    "    # Linear Regression Solution (SSE minimum) - red star #\n",
    "    reg_sol = stats.linregress(x, y)\n",
    "    ax.plot([reg_sol.slope], [reg_sol.intercept], [-200], '*r', ms=15, alpha=0.8, label='SSE min')\n",
    "    \n",
    "    ax.legend(loc=0)\n",
    "    plt.show()\n",
    "    # return b0, b1, sse_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 6 iters the optimization has converged\n",
    "plot_sse(x, y, sgd_ans[:7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
